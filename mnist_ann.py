# -*- coding: utf-8 -*-
"""Mnist_ANN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eW4nKTnTOQyLQdpTMurga2FGn3nB2ILw
"""

!pip install keras-tuner

import tensorflow as tf
from tensorflow import keras
import numpy as np
from keras.layers import Dense, Flatten
from keras.models import Sequential
import kerastuner as kt
from kerastuner import RandomSearch
from kerastuner.engine.hyperparameters import HyperParameters
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score

print(tf.__version__)

mnist=keras.datasets.mnist

(train_images,train_labels),(test_images,test_labels)=mnist.load_data()

train_images=train_images/255.0
test_images=test_images/255.0

train_images[0].shape
np.unique(test_labels)

train_images=train_images.reshape(len(train_images),28,28,1)
test_images=test_images.reshape(len(test_images),28,28,1)

def build_model(hp):  
  model = Sequential()
  model.add(Flatten(input_shape=(28,28)))
  hidden_layer=model.add(Dense( units=hp.Int(
                    'units',
                    min_value=32,
                    max_value=128,
                    step=32,
                    default=128),
        activation=hp.Choice('act_'+str(),['relu','tanh','sigmoid'],default='relu')))
  model.add(Dense(10, activation='softmax'))

  
  model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])), loss='sparse_categorical_crossentropy', metrics=['accuracy']) 

  return model

tuner=kt.Hyperband(build_model,
                          objective='val_accuracy',
                          max_epochs=5,directory='out',project_name="Auto-AF")

stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)

tuner.search(train_images,train_labels,epochs=5,validation_split=0.1)

tuner.search_space_summary()

best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f""" The optimal units in dense layer is {best_hps.get('units')} the optimal learning rate is {best_hps.get('learning_rate')} and the best activation function is {best_hps.get('act_')}""")

model=tuner.hypermodel.build(best_hps)
history =model.fit(train_images, train_labels,epochs=20, validation_split=0.1, initial_epoch=5)

history.history??

loss_train = history.history['loss']
plt.plot(loss_train, 'g', label='loss')
plt.title('loss vs epochs')
plt.xlabel('Epoch')
plt.ylabel('loss')
plt.legend()
plt.show()

y_pred=model.predict(test_images)
test_pred_labels = np.argmax(y_pred, axis=-1)

test_labels.shape

model_eval=model.evaluate(test_images,test_labels)
print("[test loss,test accuracy]:",model_eval)

model_eval??

test_loss= model_eval[0]
test_loss

from sklearn.metrics import classification_report
target_names = ['class 0', 'class 1', 'class 2','class 3','class 4','class 5','class 6','class 7','class 8','class 9']
print(classification_report(test_labels, test_pred_labels , target_names=target_names))

f1score = f1_score(test_labels, test_pred_labels, average=None)
f1score

f1_score(test_labels, test_pred_labels, average='weighted')